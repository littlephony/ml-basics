{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "Linear regression is a very basic model for solving regression problem of supervised learning.\n",
    "\n",
    "Let's define our input data:\n",
    "* let $(X, \\vec{y})$ be a **training set**.\n",
    "* $X$ is a $m \\times n$ matrix, where $m$ is a number of **observations** and $n$ is a number of **training features**.\n",
    "* $\\vec{y}$ is a vector of targets for each of $m$ observations.  \n",
    "\n",
    "Since regression is linear, we will look for the solution in the following form:\n",
    "\n",
    "$$ \\hat{y} = f_{\\vec{w}, b} (\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b $$\n",
    "\n",
    "where $\\vec{w}$ is a vector of **weights** for each feature and $b$ is **bias**.\n",
    "\n",
    "### Cost function\n",
    "\n",
    "Now that we have defined our model, we need to find a way to find the best parametrs $\\vec{w}, b$ (or very close to the best). In order to do so, we introduct a **cost function**. It's used to measure how *close* our predicted values are to the targets.\n",
    "\n",
    "$$J(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i = 1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "Since we want our model to be as good as possible, we need to minimize the cost function. In other words, we need to solve the following optimization problem:\n",
    "\n",
    "$$ J(\\vec{w}, b) = \\frac{1}{2m} \\sum_{i = 1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2 \\rightarrow min $$\n",
    "\n",
    "To solve this problem, we will use the **gradient descent** algorithm. Since the cost function is convex, we are guaranteed to find the global minimum with this approach. The calculations will be done following these formulas:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w_j} = w_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}(f_{\\vec{w}, b} (\\vec{x}^{(i)}) - y^{(i)})x_{j}^{(i)}$$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial b} = b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}(f_{\\vec{w}, b}(\\vec{x}^{(i)}) - y^{(i)})$$\n",
    "\n",
    "The parameter $\\alpha$ is called the **learning rate**. The value of $\\alpha$ should be chosen for every individual problem. If it is too large, than gradient descent may not converge. If it is too small, it may converge very slowly.\n",
    "\n",
    "### Feature scaling\n",
    "\n",
    "In practice, different feature of a problem's dataset may have vastly different ranges, e.g. some are very big while others are very small. In this case, different features will have unequal effect on the model's performance. As such, we would like to **scale** the variables so that they have more or less the same range.\n",
    "\n",
    "* **Mean normalization**\n",
    "\n",
    "    $x_j^{(i)} = \\frac{x_{j}^{(i)} - \\mu_{j}}{max-min} $\n",
    "* **z-score normalization**\n",
    "    \n",
    "    $x_j^{(i)} = \\frac{x_{j}^{i} - \\mu_{j}}{\\sigma_{j}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf073ed7ffba115fc3146f82f980a4a60308a85f87d4b5344ff0791a491b50c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
